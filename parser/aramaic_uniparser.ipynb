{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find words in MASC_data which match a pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels=\"aouieāōūīēə\"\n",
    "ac_vowels=\"áíéóú\"\n",
    "\n",
    "KuuK_m_pattern=\"^[^aouieāōūīēə]ū[^aouieāōūīēə]a$\"\n",
    "KuuK_f_pattern=\"^[^aouieāōūīēə]ū[^aouieāōūīēə]ča$\"\n",
    "KuuK_f1_pattern=\"^[^aouieāōūīēə]ū[^aouieāōūīēə]ṯa$\"\n",
    "\n",
    "KiiK_m_pattern=\"^[^aouieāōūīēə]ī[^aouieāōūīēə]a$\"\n",
    "KiiK_f_pattern=\"^[^aouieāōūīēə]ī[^aouieāōūīēə]ča$\"\n",
    "KiiK_f1_pattern=\"^[^aouieāōūīēə]ī[^aouieāōūīēə]ṯa$\"\n",
    "\n",
    "KooK_m_pattern=\"^[^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "KooK_f_pattern=\"^[^aouieāōūīēə]ō[^aouieāōūīēə]ča$\"\n",
    "KooK_f1_pattern=\"^[^aouieāōūīēə]ō[^aouieāōūīēə]ṯa$\"\n",
    "\n",
    "KeeK_m_pattern=\"^[^aouieāōūīēə]ē[^aouieāōūīēə]a$\"\n",
    "KeeK_f_pattern=\"^[^aouieāōūīēə]ē[^aouieāōūīēə]ča$\"\n",
    "KeeK_f1_pattern=\"^[^aouieāōūīēə]ē[^aouieāōūīēə]ṯa$\"\n",
    "\n",
    "KKuu_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ū$\"\n",
    "KKii_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]īṯa$\"\n",
    "KKoo_m_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ō$\"\n",
    "KKoo_f_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ōṯa$\"\n",
    "\n",
    "KiKKa_pattern=\"^[^aouieāōūīēə]i[^aouieāōūīēə][^aouieāōūīēə]a$\"\n",
    "KuKKa_pattern=\"^[^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]a$\"\n",
    "KaKKa_m_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a$\"\n",
    "KaKKa_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə]ə[^aouieāōūīēə]ṯa$\"\n",
    "KeKKa_m_pattern=\"^[^aouieāōūīēə]e[^aouieāōūīēə][^aouieāōūīēə]a$\"\n",
    "KeKKa_f_pattern=\"^[^aouieāōūīēə]e[^aouieāōūīēə]ə[^aouieāōūīēə]ṯa$\"\n",
    "KoKKa_m_pattern=\"^[^aouieāōūīēə]o[^aouieāōūīēə][^aouieāōūīēə]a$\"\n",
    "KoKKa_f_pattern=\"^[^aouieāōūīēə]o[^aouieāōūīēə]ə[^aouieāōūīēə]ṯa$\"\n",
    "\n",
    "KKaKta_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə](ṯ|č)a$\"\n",
    "KKeKta_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə](ṯ|č)a$\"\n",
    "KKoKta_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]o[^aouieāōūīēə](ṯ|č)a$\"\n",
    "KKiKta_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]i[^aouieāōūīēə](ṯ|č)a$\"\n",
    "KKuKta_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]u[^aouieāōūīēə](ṯ|č)a$\"\n",
    "\n",
    "KvKvKv_pattern=\"^[^aouieāōūīēə][áíéóú][^aouieāōūīēə][aouieāōūīē][^aouieāōūīēə][aouieāōūīē]$\"\n",
    "\n",
    "KKooKa_m_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēəṯ]a$\"\n",
    "KKiiKa_m_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ī[^aouieāōūīēəṯ]a$\"\n",
    "KKuuKa_m_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ū[^aouieāōūīēəṯ]a$\"\n",
    "KKeeKa_m_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ē[^aouieāōūīēəṯ]a$\"\n",
    "KKooKa_f_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə]ṯa$\"\n",
    "KKooKa_f1_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə]ča$\"\n",
    "KKiiKa_f_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ī[^aouieāōūīēə]ṯa$\"\n",
    "KKiiKa_f1_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ī[^aouieāōūīēə]ča$\"\n",
    "KKuuKa_f_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ū[^aouieāōūīēə]ṯa$\"\n",
    "KKuuKa_f1_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]ū[^aouieāōūīēə]ča$\"\n",
    "\n",
    "KooKKa_m_pattern=\"^[^aouieāōūīēə]ō[^aouieāōūīēə][^aouieāōūīēəṯč]a$\"\n",
    "KooKKa_f_pattern=\"^[^aouieāōūīēə]ō[^aouieāōūīēə]ə[^aouieāōūīēə]ṯa$\"\n",
    "KooKKa_f1_pattern=\"^[^aouieāōūīēə]ō[^aouieāōūīēə]ə[^aouieāōūīēə]ča$\"\n",
    "\n",
    "KeeKKa_m_pattern=\"^[^aouieāōūīēə]ē[^aouieāōūīēə][^aouieāōūīēəṯč]a$\"\n",
    "KeeKKa_f_pattern=\"^[^aouieāōūīēə]ē[^aouieāōūīēə]ə[^aouieāōūīēə]ṯa$\"\n",
    "KeeKKa_f1_pattern=\"^[^aouieāōūīēə]ē[^aouieāōūīēə]ə[^aouieāōūīēə]ča$\"\n",
    "\n",
    "KaKooKa_m_pattern=\"^[^aouieāōūīēə][a][^aouieāōūīēə][ōū][^aouieāōūīēə]a$\"\n",
    "KaKiiKa_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə]ī[^aouieāōūīēə][ṯč]a$\"\n",
    "KaKiiKa_m_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə]ī[^aouieāōūīēə]a$\"\n",
    "KaKooKa_f_pattern=\"^[^aouieāōūīēə][a][^aouieāōūīēə][ōū][^aouieāōūīēə][ṯč]a$\"\n",
    "KiuKooKa_f_pattern=\"^[^aouieāōūīēə][iu][^aouieāōūīēə]ō[^aouieāōūīēə][ṯč]a$\"\n",
    "KaKiiKa_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə]ī[^aouieāōūīēə][ṯč]a$\"\n",
    "\n",
    "KaKKooKa_m_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "KuKKooKa_m_pattern=\"^[^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "KiKKooKa_m_pattern=\"^[^aouieāōūīēə]i[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "KaKKuuKa_m_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]ū[^aouieāōūīēə]a$\"\n",
    "KaKKiiKa_m_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]ī[^aouieāōūīēə]a$\"\n",
    "KiKKiiKa_m_pattern=\"^[^aouieāōūīēə]i[^aouieāōūīēə][^aouieāōūīēə]ī[^aouieāōūīēə]a$\"\n",
    "KaKKeeKa_m_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]ē[^aouieāōūīēə]a$\"\n",
    "KaKKooKa_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə][ṯč]a$\"\n",
    "KuKKooKa_f_pattern=\"^[^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə][ṯč]a$\"\n",
    "KiKKooKa_f_pattern=\"^[^aouieāōūīēə]i[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə][ṯč]a$\"\n",
    "KaKKuuKa_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]ū[^aouieāōūīēə][ṯč]a$\"\n",
    "KaKKiiKa_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]ī[^aouieāōūīēə][ṯč]a$\"\n",
    "KiKKiiKa_f_pattern=\"^[^aouieāōūīēə]i[^aouieāōūīēə][^aouieāōūīēə]ī[^aouieāōūīēə][ṯč]a$\"\n",
    "KaKKeeKa_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]ē[^aouieāōūīēə][ṯč]a$\"\n",
    "\n",
    "KaKKKa_m_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə]ə?[^aouieāōūīēə][^aouieāōūīēəṯč]a$\"\n",
    "KuKKKa_m_pattern=\"^[^aouieāōūīēə]u[^aouieāōūīēə]ə?[^aouieāōūīēə][^aouieāōūīēəṯč]a$\"\n",
    "KKuKKta_f_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]u[^aouieāōūīēə]ə?[^aouieāōūīēə][ṯč]a$\"\n",
    "KKeKKta_f_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə]ə?[^aouieāōūīēə][ṯč]a$\"\n",
    "KKoKKta_f_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]o[^aouieāōūīēə]ə?[^aouieāōūīēə][ṯč]a$\"\n",
    "\n",
    "KaKKaKta_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə][ṯč]a$\"\n",
    "KuKKuKta_f_pattern=\"^[^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]u[^aouieāōūīēə][ṯč]a$\"\n",
    "KuKKaKta_f_pattern=\"^[^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə][ṯč]a$\"\n",
    "KuKKoKta_f_pattern=\"^[^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]o[^aouieāōūīēə][ṯč]a$\"\n",
    "KiKKeKta_f_pattern=\"^[^aouieāōūīēə]i[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə][ṯč]a$\"\n",
    "KaKKuKta_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]u[^aouieāōūīēə][ṯč]a$\"\n",
    "KiKaKeKta_f_pattern=\"^[^aouieāōūīēə]i[^aouieāōūīēə]a[^aouieāōūīēə]e[^aouieāōūīēə][ṯč]a$\"\n",
    "KaKKeKta_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə][ṯč]a$\"\n",
    "KKaKaKta_f_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]a[^aouieāōūīēə][ṯč]a$\"\n",
    "KaKeKta_f_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə]e[^aouieāōūīēə][ṯč]a$\"\n",
    "\n",
    "KaKaKooKa_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə]a[^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "KaKaKeeKa_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə]a[^aouieāōūīēə]ē[^aouieāōūīēə]a$\"\n",
    "\n",
    "KuKooKa_pattern=\"^mu[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "KuKooKta_pattern=\"^mu[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə][ṯč]a$\"\n",
    "KKaKKa_m_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a$\"\n",
    "KKaKKa_f_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]ə?[^aouieāōūīēə][ṯč]a$\"\n",
    "\n",
    "fünfradikale1=\"^[^aouieāōūīēə][aouieāōūīēə][^aouieāōūīēə][^aouieāōūīēə][aouieāōūīēə][^aouieāōūīēə][^aouieāōūīēəṯč]a$\"\n",
    "fünfradikale2=\"^[^aouieāōūīēə][aouieāōūīēə][^aouieāōūīēə][^aouieāōūīēə][aouieāōūīēə][^aouieāōūīēə][āōūīē][^aouieāōūīēəṯč]a$\"\n",
    "fünfradikale3=\"^[^aouieāōūīēə][^aouieāōūīēə][aouieāōūīēə][^aouieāōūīēə][^aouieāōūīēə][aouieāōūīēə][^aouieāōūīēəṯč]a$\"\n",
    "\n",
    "KiKKooKKa_pattern=\"^mi[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə][ṯč]a$\"\n",
    "KKaKKaKKa_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə][ṯč]a$\"\n",
    "KKuKKa_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]a$\"\n",
    "iKKiKooKa_pattern=\"^i[^aouieāōūīēə][^aouieāōūīēə]i[^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "iKKiKKooKa_pattern=\"^i[^aouieāōūīēə][^aouieāōūīēə]i[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "KuKKaKKaK_pattern=\"^[^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "KiKKKooKa_pattern=\"^[^aouieāōūīēə]i[^aouieāōūīēə][^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "KaKKaKooKa_pattern=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "nita_pattern=\"^.*nīṯa$\"\n",
    "ita_pattern=\"^.*īṯa$\"\n",
    "onca_pattern=\"^.*onča$\"\n",
    "uta_pattern=\"^.*ūṯa$\"\n",
    "oyta_pattern=\"^.*ōyṯa$\"\n",
    "KKaKKKooKa_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "KKaKKooKa_pattern=\"^[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]ō[^aouieāōūīēə]a$\"\n",
    "\n",
    "\n",
    "verb_I_1=\"^i[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə] yi[^aouieāōūīēə][^aouieāōūīēə]u[^aouieāōūīēə]$\"\n",
    "verb_I_2=\"^i[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə] yi[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_I_3=\"^i[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə] yi[^aouieāōūīēə][^aouieāōūīēə]u[^aouieāōūīēə]$\"\n",
    "verb_I_4=\"^i[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə] yi[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_II_1=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə] y[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_II_2=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə] y[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə]$\"\n",
    "verb_II_2=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə] y[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə]$\"\n",
    "verb_III_1=\"^[^aouieāōūīēə]ō[^aouieāōūīēə]e[^aouieāōūīēə] y[^aouieāōūīēə]ō[^aouieāōūīēə]e[^aouieāōūīēə]$\"\n",
    "verb_III_2=\"^[^aouieāōūīēə]ō[^aouieāōūīēə]a[^aouieāōūīēə] y[^aouieāōūīēə]ō[^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_IV_1=\"^a[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə] ya[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə]$\"\n",
    "verb_IV_2=\"^a[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə] ya[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_II2=\"^č[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə] yič[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_III2=\"^č[^aouieāōūīēə]ō[^aouieāōūīēə]a[^aouieāōūīēə] yič[^aouieāōūīēə]ō[^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_I7=\"^inə?[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə] yinə?[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_I8_1=\"^i[^aouieāōūīēə]ə?č[^aouieāōūīēə]e[^aouieāōūīēə] yi[^aouieāōūīēə]ə?č[^aouieāōūīēə]e[^aouieāōūīēə]$\"\n",
    "verb_I8_2=\"^i[^aouieāōūīēə]ə?č[^aouieāōūīēə]a[^aouieāōūīēə] yi[^aouieāōūīēə]ə?č[^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_I10_1=\"^sča[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə] yisča[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə]$\"\n",
    "verb_I10_2=\"^sča[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə] yisča[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_Iyw=\"^i[^aouieāōūīēə]e[^aouieāōūīēə] yī[^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_Ialeph=\"^a[^aouieāōūīēə]a[^aouieāōūīēə] yī[^aouieāōūīēə]u[^aouieāōūīēə]$\"\n",
    "verb_amet=\"^a[^aouieāōūīēə]e[^aouieāōūīēə] yī[^aouieāōūīēə]u[^aouieāōūīēə]$\"\n",
    "verb_oseg=\"^ō[^aouieāōūīēə]e[^aouieāōūīēə] yō[^aouieāōūīēə]e[^aouieāōūīēə]$\"\n",
    "verb_intar=\"^in[^aouieāōūīēə]a[^aouieāōūīēə] yin[^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "verb_iscak=\"^i[^aouieāōūīēə]ča[^aouieāōūīēə] yi[^aouieāōūīēə]ča[^aouieāōūīēə]$\"\n",
    "verb_IIIy=\"^i[^aouieāōūīēə]ə[^aouieāōūīēə] yi[^aouieāōūīēə]ə[^aouieāōūīēə]$\"\n",
    "verb_IVy=\"^a[^aouieāōūīēə]ə[^aouieāōūīēə] ya[^aouieāōūīēə]ə[^aouieāōūīēə]$\"\n",
    "verb_ayt=\"^a[^aouieāōūīēə][^aouieāōūīēə]\\(i\\) ya[^aouieāōūīēə][^aouieāōūīēə]\\(i\\)$\"\n",
    "verb_isw=\"^i[^aouieāōūīēə][^aouieāōūīēə].* yi[^aouieāōūīēə][^aouieāōūīēə].*$\"\n",
    "\n",
    "\n",
    "adj_iKKuK=\"^i[^aouieāōūīēə][^aouieāōūīēəʕ]u[^aouieāōūīēə]$\"\n",
    "adj_iKKuK1=\"^i[^aouieāōūīēə]ʕu[^aouieāōūīēə]$\"\n",
    "adj_iKKeK=\"^i[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə]$\"\n",
    "adj_KuKKaK=\"^[^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "adj_KuKKuK=\"^[^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]u[^aouieāōūīēə]$\"\n",
    "adj_KaKKeK=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]e[^aouieāōūīēə]$\"\n",
    "adj_KooKaK=\"^[^aouieāōūīēə]ō[^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "adj_maKKuK=\"^ma[^aouieāōūīēə][^aouieāōūīēə]u[^aouieāōūīēə]$\"\n",
    "adj_mKaKKaK=\"^m[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]a[^aouieāōūīēə]$\"\n",
    "adj_KaKKan=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]an$\"\n",
    "adj_KuKKen=\"^[^aouieāōūīēə]u[^aouieāōūīēə][^aouieāōūīēə]en$\"\n",
    "adj_KiKKen=\"^[^aouieāōūīēə]i[^aouieāōūīēə][^aouieāōūīēə]en$\"\n",
    "adj_ay=\"^.*ay$\"\n",
    "adj_imet=\"^i[^aouieāōūīēə]e[^aouieāōūīēə]$\"\n",
    "adj_tabb=\"^[^aouieāōūīēə]a[^aouieāōūīēə][^aouieāōūīēə]$\"\n",
    "adj_ikw=\"^i[^aouieāōūīēə]ə?[^aouieāōūīēə]$\"\n",
    "adj_gool=\"^[^aouieāōūīēə]ō[^aouieāōūīēə]$\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12225 entries, 0 to 12224\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Root       12225 non-null  object\n",
      " 1   Lemma      12225 non-null  object\n",
      " 2   LemmaFreq  12225 non-null  int64 \n",
      " 3   Word_form  12225 non-null  object\n",
      " 4   Freq       12225 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 477.7+ KB\n"
     ]
    }
   ],
   "source": [
    "masc_data=pd.read_csv(\"MASC_data.csv\", sep=\";\")\n",
    "masc_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_list=masc_data[\"Lemma\"].tolist()\n",
    "wordforms_list = masc_data['Word_form'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_patterns(patt):\n",
    "    matched_lemmas=[]\n",
    "    for i in range(len(lemmas_list)):\n",
    "        match = re.search(patt, lemmas_list[i])\n",
    "        if match:\n",
    "            matched_lemmas.append(lemmas_list[i])\n",
    "    matched_df=masc_data[(masc_data[\"Lemma\"].isin(matched_lemmas))]\n",
    "    return matched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15 entries, 4686 to 11627\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Root       15 non-null     object\n",
      " 1   Lemma      15 non-null     object\n",
      " 2   LemmaFreq  15 non-null     int64 \n",
      " 3   Word_form  15 non-null     object\n",
      " 4   Freq       15 non-null     int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 720.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "K_df=search_patterns(KiKKa_pattern)\n",
    "print(K_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Root</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>LemmaFreq</th>\n",
       "      <th>Word_form</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4686</th>\n",
       "      <td>šmš</td>\n",
       "      <td>šimša</td>\n",
       "      <td>37</td>\n",
       "      <td>šimša</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4816</th>\n",
       "      <td>yḏʕ</td>\n",
       "      <td>činya</td>\n",
       "      <td>34</td>\n",
       "      <td>činya</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5136</th>\n",
       "      <td>bl</td>\n",
       "      <td>billa</td>\n",
       "      <td>27</td>\n",
       "      <td>billa</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>šḳy</td>\n",
       "      <td>šiḳya</td>\n",
       "      <td>19</td>\n",
       "      <td>šiḳya</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5842</th>\n",
       "      <td>šḳy</td>\n",
       "      <td>šiḳya</td>\n",
       "      <td>19</td>\n",
       "      <td>šiḳyil</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Root  Lemma  LemmaFreq Word_form  Freq\n",
       "4686  šmš  šimša         37     šimša    37\n",
       "4816  yḏʕ  činya         34     činya    34\n",
       "5136   bl  billa         27     billa    27\n",
       "5841  šḳy  šiḳya         19     šiḳya    18\n",
       "5842  šḳy  šiḳya         19    šiḳyil     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "šimša\n",
      "činya\n",
      "billa\n",
      "šiḳya\n",
      "riḥya\n",
      "žitta\n",
      "žičča\n",
      "miḥya\n",
      "sičča\n"
     ]
    }
   ],
   "source": [
    "K_lemmas=K_df.Lemma.unique()\n",
    "for K in K_lemmas:\n",
    "    print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lexemes(lemmata):\n",
    "    patt_lemmas=lemmata\n",
    "    f=open(\"lexemes.txt\", \"a\")\n",
    "    for patt_lemm in patt_lemmas:         \n",
    "        f.write(\"-lexeme\\n\")\n",
    "        f.write(\" lex: \"+patt_lemm+\"\\n\")\n",
    "        f.write(\" stem: \"+patt_lemm[:-1]+\".\\n\")\n",
    "        f.write(\" gramm: NOUN,m\\n\")\n",
    "        f.write(\" paradigm: NOUN_KiKKa\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_lexemes(K_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UniParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uniparser_morph in /home/sofia1236/.local/lib/python3.8/site-packages (2.7.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install uniparser_morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uniparser_morph import Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Analyzer()\n",
    "a.load_grammar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Wordform object>\n",
      "ġamlax\n",
      "ġamla; NOUN,POSS.2ms,Sg,m\n",
      "ġaml-ax\n",
      "STEM=2ms\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "analyses=a.analyze_words(\"ġamlax\")\n",
    "print(analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordform:  ġamlax\n",
      "lemma:  ġamla\n",
      "grammatical features:  NOUN,m,Sg,POSS.2ms\n",
      "glossed wordform:  ġaml-ax\n",
      "glosses:  STEM=2ms\n"
     ]
    }
   ],
   "source": [
    "for ana in analyses:\n",
    "      print(\"wordform: \", ana.wf)\n",
    "      print(\"lemma: \", ana.lemma)\n",
    "      print(\"grammatical features: \", ana.gramm)\n",
    "      print(\"glossed wordform: \", ana.wfGlossed)\n",
    "      print(\"glosses: \", ana.gloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tana\tana\tPRON\tpron_type=Pers|person=1|number=Sg\tana\tI\n",
      "2\txett\txett\tADV\tadv_type=Mod\txett\tSTEM\n",
      "3\tbatt\tabət yabət|batte\tAUX|VERB\tStamm=IVy|tense=Perf|person=3|number=Sg|gender=m | person=1|number=Sg|gender=c\tbatt\tFUT|STEM\n",
      "4\tṯinaġelča\tṯinaġelča\tNOUN\tgender=f|number=Sg\tṯinaġel-č-a\tSTEM-F-FREE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyses = a.analyze_words(['ana', 'xett', 'batt', 'ṯinaġelča'], format=\"conll\")\n",
    "print(analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If do not have prepared list of words, you can use this function to get it from text strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tok(text_to_gloss):\n",
    "    first_tokens = nltk.word_tokenize(text_to_gloss)\n",
    "    final_tokens=[]\n",
    "    for tok in first_tokens:\n",
    "        tok=tok.lower()\n",
    "        if \"-\" in tok:\n",
    "            final_tokens.extend(tok.split(\"-\"))\n",
    "        elif \"_\" in tok:\n",
    "            final_tokens.extend(tok.split(\"_\"))\n",
    "        elif tok.isalpha() or \"ḏ̣\" in tok:\n",
    "            final_tokens.append(tok)\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare gold annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations=[]\n",
    "g=open(\"annotation.txt\", \"r\", encoding=\"utf-8\")\n",
    "ann_sentence=[]\n",
    "for line in g:\n",
    "    if line.startswith(\"0\") or line.startswith(\"3\"):\n",
    "        if len(ann_sentence)>0:\n",
    "            annotations.append(ann_sentence)\n",
    "        ann_sentence=[]\n",
    "    if \"\\t\\t\" in line:\n",
    "        help_ann=[]\n",
    "        line_splitted=line.split(\"\\t\\t\")[:3]\n",
    "        if line_splitted[1]!=\"EMPTY\":\n",
    "            help_ann.append(line_splitted[0][:-1])\n",
    "            help_ann.append(line_splitted[1])\n",
    "            grams=line_splitted[-1].split(\",\")\n",
    "            help_ann.append(grams[0])\n",
    "            help_ann.append(set(grams[1:]))\n",
    "            ann_sentence.append(help_ann)\n",
    "        \n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ē', 'ē', 'PART', set()], ['čbōʕin', 'ibəʕ yibəʕ', 'VERB', {'I', 'Pl', '2', 'Prs', 'm'}], ['nišw', 'išw yišw', 'VERB', {'I', 'Pl', 'Subj', 'c', '1'}], ['ḥenna', 'ḥenna', 'NOUN', {'Sg', 'm'}]]\n"
     ]
    }
   ],
   "source": [
    "print(annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_annotation(gold_annotation):\n",
    "    \n",
    "    g=open(gold_annotation, \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    annotations_non_empty=[]\n",
    "    annotations_all=[]\n",
    "    annotations_non_empty_unique=[]\n",
    "    annotations_all_unique=[]\n",
    "    \n",
    "    for line in g:        \n",
    "        \n",
    "        if \"\\t\\t\" in line:\n",
    "            help_ann=[]\n",
    "            line_splitted=line.split(\"\\t\\t\")[:3]\n",
    "            help_ann.append(line_splitted[0][:-1])\n",
    "            help_ann.append(line_splitted[1])\n",
    "            grams=line_splitted[-1].split(\",\")\n",
    "            help_ann.append(grams[0])\n",
    "            help_ann.append(set(grams[1:]))\n",
    "            annotations_all.append(help_ann)\n",
    "            if help_ann not in annotations_all_unique:\n",
    "                annotations_all_unique.append(help_ann)\n",
    "            \n",
    "            help_ann2=[]\n",
    "            if line_splitted[1]!=\"EMPTY\":\n",
    "                help_ann2.append(line_splitted[0][:-1])\n",
    "                help_ann2.append(line_splitted[1])\n",
    "                grams=line_splitted[-1].split(\",\")\n",
    "                help_ann2.append(grams[0])\n",
    "                help_ann2.append(set(grams[1:]))\n",
    "                annotations_non_empty.append(help_ann2)\n",
    "                if help_ann2 not in annotations_non_empty_unique:\n",
    "                    annotations_non_empty_unique.append(help_ann2) \n",
    "    g.close()\n",
    "    \n",
    "    return annotations_all, annotations_all_unique, annotations_non_empty, annotations_non_empty_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1,ann2,ann3,ann4=get_gold_annotation(\"annotation.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of sents on which check is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "sents_ann=['005. «ē, čbōʕin nišw ḥenna?»', '014. bōṯar irpiʕ yūm mʕawītin xett mayṯyin ḏbīḥča, naxsilla.', '0. ḳarribōye ḳaʕyin b-anna payṯa, mayṯyin ḳurʔān w ḳaryill lanna ḳurʔān mn-awwalče l-axerče.', '016. ḳōymin ommṯa bakkar, nōḥčin ʕa žēmʕa ʕam-ṣallyin w zlillun ʕa žappōnča.', '003. zlallen ʕṣofra yzūran.', '004. hū čū ōxel menna, mōrəl ḏ̣ḥīṯa ču maḥiḳḳle yīxul, yḏuḳenne bnaw bnawb, yīxul menna.', '009. amrulla: «nḥūč!» niḥčaṯ, niḥčaṯ ʕal-ōḏ mʕarrṯa.', '004. ḳaʕya willa la iḥmaṯ illa iʕber aʕla, iʕber aʕla, iḏʕaṯ hī.', '015. ʕala ṭūl mičwaḥḥ bē w ʕala ṭūl p-ḥaṣṣir rayše w iṭleb menne metti w batte yapplēle.', '003. ōṯ ġabrōna, wōb b-ġayrib blōta w ōṯ.', '001. b-zamōne ṯōle aḥḥaḏ dumōnay l-ōxa, ōmar mbaḳḳar p-šaġəlṯil mōya p-ḳīsa.', '016. ē, lōmar, la karr, aḳam ōčem ikbet b-ḏokkṯe w iḳəʕ, w hōṯa mafizza.', '004. b-zamōna ʕaččīḳa msawəlfin wōṯ ḏēba rōfeḳ ṯaʕla malʕun w inžes w šūne lēle rfīḳa ġassem maʕ ḏibō ti iṯḳen mamrille yičwaʕʕ menne w yabʕeḏ meʕle.', '028. mʕapparlun ṯaʕla ʕa mahle w ōxel, w minəpṣaṭ hū w bnōye w eččṯe.', '007. b-anna tarba allxinnaḥ, // w meʕle lorkaʕ ʕawitinnaḥ. // p-ḥaṣṣiš šenna šaʕʕlinnaḥ, // w ʕa šenna irčfaʕ ṣlība.', '003. b-ʕaynōš nḥamēl kawna. // ʕaya la amrīšəl b-riḥmūṯiš? // ʕaya ḏikkliš aʕəl, amrīšəl: čimʕawīta. // hašš warṯṯa, ʕaya la affīšən nzurʕenna b-lipp?', '004. inəbraḏ — ext bann nišwi? // bann nišči ḳūrəl mišwi, // w mina mann nayṯi ḥamra?', '001. ḥmičča sallīḳa ʕa ḏayra // w zannīra b-zunnōrəs sayra // w lippi la irḥam ġayra // w hōḏ raḥme, berčis sōba.', '004. amrōli: «payṯaḥ iʕli». // amrilla: «p-ḥaṣṣe nṭayyer.»', '002. hōš nizlōli ʕa payṯa w niṯyōli, // immi p-tarba hōši minčaḳyōli. // hačči l-ḥōlax zellax w_ana l-ḥōli, // w ʕrōba čḳōmez ʕa payṯa ḳmōza.', '004. awʕax čīmar: batt w batt // mn-ōt tunya, tunya mīṯa. // barnōša ʕa ḥōle mʕatt // mn-īḏe isḳaṭ bə-ḥṭīṯa.', '012. yīfuš ti čṯēx l-ġappe, // čmiščaḥ ġawza w pšōṯa, // ōxel w mišwi b-ʕoppe, // w šōḳel m-maḥərmōṯa.', '011. xēfəl matōra ḳōtrin ʕa baġla w mafčel ḥetta yṯuḳnun ex ḥmīra.', '006. maḳimill lanna nšīfa w mawḳfin maʕ ġrōsa, ṯyillun atar ġayrayy, xett šappō, ḳaʕyillun ʕal-ōġ ġrōrča, ġōrsin w mʕannyin w masəkrin w mbaṣṭin.', '018. čūb ext_imōḏ, imōḏ ḥarīma baḥar čannīḥan, awwalča ḥarīma baḥar čaʕbōnan.', '009. nafḏiṯ l-ellel, niḥčiṯ ʕa mʕarrṯa, lā ōṯ ʕimm nohra w lā ōṯ ʕimm mett.', '015. ḳōmiṯ zlill ʕa napka.', '009. w ḥrēna žabḏunne mōya ḥetta amṭunne r-reḥyil ḳamṣa l-awwalčiš šiḳya.', '010. ḏukkil ataḳ mōyər rayša, amrilla: «wuš ḳʕāš niḥəm!»', '005. mišwille b-ann finžanō, mišwille ʕa ffōye ġawzō másalan mšakklille, w mḏ̣ayīfin bē.', '028. «nōz nišč nofəšṯa.»', '016. nmišwille ʕal-anna ġāz, w nmayṯyin kuppōyṯa w nmayṯyin maṣṣōṣča w nmayṯyin ʕolpṯis sukker w nmišwillen ḳummaynaḥ.', '005. xaṭərṯa aʕzmannaḥ ḥōl nislaḳ leʕle ʕa mazraʕṯa nsaʕitenne p-ḳulḳās w filō.', '001. xarma raḏyille xanūnay w raḏyille ʕamlay.', '009. w ōbəl xalil ʕam-nayyarəl ʕaraḳ w əl-māza, w ayṯull lanna mšammaʕ w fartunne ʕa mluṯṯil lōḏ sahəlṯa.', '005. nmišwēle p-ḳoffṯa w nkamarle ḥamša šečča yūm.', '018. amrille: «lā, bann nzill.»', '003. awwalča miččažʕin ommṯa hōxa bə-blōta — čūṯ ḥkimō.', '016. ṯill ayṯiččil lōx xšurīṯa w zabničča, w ayṯiṯ maḳəḏḥa, w ṯiḳniṯ nmišwēla ḳiḏḥō ḳiḏḥō l-ōx xšurīṯa.', '006. baʕdēn nḳatrilla m-ṭarfa p-ḥūṭa.', '090. ʕayniṯ ana, aʕžbačč ʕezza, nḳōyem mamelle: «hōʕ ʕezza p-ṯmēn warḳan. ayṯa kmōlča!»', '010. ōmar: «wrāx, la yʕaṣṣrunnaḥ!»', '008. amrille: «wrāx, ana ču nfakker nḳaṭʕiṯ ʕa tarba ġēr l-emmat šarikiččax.»', '039. bess yḥassel ramla xulle sawa, yīb iṯḳen felkiš šaʕṯa, yaʕni ʕisər w ḥammeš miṣrōyan.', '004. w ġufərlēḥ ḥṭiyōṯaḥ, // ex min nġafərlill ti maḥṭ ʕimmaynaḥ.', '003. mayṯyin ḥiṭṭō w xušnō w sʕarō w zarʕillun p-ṣaḥnō zʕūrin xann, ḥetta yirbun.', '014. rēbeʕ ḥašoppa m-ṣawma ušme ḥašoppa ti zahra w warta.', '009. bess yḥassel xenša w čḥassel ṣlōṯa, xett mražīʕin tōpkin ext_awwal yōma lə-ʕrōba.', '008. m-mar lawandīyus ṣlōṯa l-ʕāde.', '003. hanna yōma, yōmlə sčašhet bē hanna ḳattēša šwunne ʕēḏa lēle.', '045. ṯōḳen tōḳeḳ naḳōsəl mar_ilyas w naḳōsəl berkṯa w ġayre w sawōye l-ḥāṣlo.', '007. hann mašəʕlō ṯōḳnin ḳīsa, ṭūle mett iṯər mičər, šammen xann ḳalles, w b-rayšil ḳīsa ōṯ ʕolpṯa m-ḥatīta aw ṣōža.', '035. w hōxa atar mballeš... šaġlōṯa ḥrinyōṯa.', '004. mḥáttitin yōma ti batte yišwull awwal ʔurbāne bē.', '033. čbaḳḳaṯ xann ṭūlčil ḥayōṯa.', '005. ōmar: «ē!»', '013. «ču nimkarr, imeṯ ḏōḏ.»', '011. amerlun: «ḥmōn hōxa ġapplə šbabō!»', '004. ōmar: «ʕṣofra.»', '010. «hačč mō čayyeṯ?»', '012. aḳam alṭun rayḥan batte yṣōraʕ hū w ayyub.', '003. anaḥ šimʕinnaḥ keləmṯa, affiččun ču tayyirill balayy w ḳōmiṯ zlill ščiččil ḥamra.', '001. xaṭərṯa nībin nsallīḳin m-dūma, nassīḳin banadōra mxaramča l-šarāb.', '002. bess ḥmiččun ana — sakrōnin —, ḳōmiṯ sakkriṯ w šamṭiṯ minnayy.', '011. w haṯinn aʕleḳ b baʕḏ̣inn w anaḥ hōxa nḏ̣ōḥkin.', '040. batte yizʕuḳ «yā ʕaḏra», lorkaʕ infeḳ ḥesse.', '001. wōṯ l-aḥḥaḏ ġabrōna eččṯa, čuṯ šunīṯa aḥla menna b-ʕōlma.', '004. yōma mn-ann yumō infeḳ ōbəl yawse ʕa ʕarḳūba ti baʕʕeḏ ḥetta yayṯ ḳalles ḏlūḳa, ḳalles xšūra, willa išmeʕ ḥessa.', '354. «ana mann napplēle!»', '091. amrōle: «ana ḳiṣəṯ xāāānn xāāānn xāāānn. affḳanni bnōṯəl ḥōlčax, aḳʕanni p-šimša.', '035. amrilla: la, atʕāy aʕle!', '003. amella: «mō bann niščġel?»', '012. wōb ʕam-fakkar yḳuṭlenne, bess la irəṣ.', '021. amrōla: «uxxul ma čmaḥəḳya čaffeḳ m-ṯimmiš žawəhrōṯa!»', '007. sčahtaṯ ʕal_eḥḏa, ḳōyma hōḏ amralla: «ṭalpiš ġapp ana. šarṭōš hann arpʕa ana nwaffīḳa ʕlayy w nraṣṣīya bōn.»', '012. išw semla w iḳlab, willa ḳaʕya elġul mapṣūṭ, amella: «wrēš mō ḳiṣṣṯiš?»', '025. aḳam applēle.', '017. aḳam ʕṣofra ščḥunne arḥel.', '015. taššrunne w zalle.', '012. isleḳ lahhīyin ʕam-mōxlin.', '004. bōṯar čiḳrīban felkiš šaʕṯa aḳa ḥōne ščafəḳte.', '010. bess hū m-zaʕle w ʕemmiš šaṯṯ w hōṯe ičbar p-xōṭre, miḥəl ḏrōʕe mett yaffeḳ besra, aḳam ameṯ.', '014. amrulle: «čūṯ ġappaynaḥ.»', '023. ʕōbra, w surtōbiš šenna rabbi.', '033. ḏukkil iṭlab xōla, aḳam hann ḥkimō čʕažžab lə-brōm wakīn.', '021. amelle: «fōk hann ḥablō m-ʕal-anna ġešra!»', '007. ḳʕōle mišṭaʕ b-iṯər sayf, ṭarḳil īḏe p-ḥelsil awwal aḥḥaḏ w ōfez, ḳaṭṭaʕ maʕ šobʕa ḥōḏ, maʕ šobʕa ḥmōr.', '003. zōyʕin menne ommṯa, hū ižreʕ, ču mahemmle.', '003. aḳa hanna šappa batte yṣaḥḥenna, amella: «ḳūm yḳuṭʕell ʕumriš! ana ʕanmōzaḥ ʕimmiš.»', '005. baʕdēn illa naffeḏ aḥḥaḏ mažnun xwōṯe amellun: «ana nmaḥḥečle.»', '001. ōṯ ġabrōna m-ḥilpul, ġappe psōna ʕomre uppe eʕsar išən.', '004. xaṭpa w kallel aʕla.', '003. waḳčil ḥimne ḥkīma xann, amelle: «xalaṣ, hačči ṯiḳnič kayyes, šoppṯa ḥrīṯa bann naffennax čzellax ʕa tiḏōx, lōfaš uppax mett.»', '012. amelle: «ʕalle mʕallaḳ ʕa ṯarʕit tikkōna!»', '011. waxma taḳḳe w naʕʕme w šūne ʕa ffōyəl ʕolpṯa w zalle l-ʕa harūn ər-rašīd.', '004. amrōle: «ē, ṭabʕan, nyaḏḏīʕa innu ṯiḳninnaḥ šaxṣa aḥḥaḏ, bess la činəš čiṭlub xōla l-iṯər!»', '010. aḥref ebre zʕōra, amelle: «ana nhamešle w nmaṣeṣle w ntaḳeḳle w nsafefle.»', '011. amelle: «ana nʔažərlēle ḥmōra nʔažərlēle fayya?»', '003. maʕzmōle: «čfaḏ̣ḏ̣āl yā ḳašīša, našḳennax ḳahwe, čfaḏ̣ḏ̣āl!»', '005. W-ḥayyil l-ann ʕaynō! Čūl meʕle ġnō. // Ex ūle leppa yiṣlinni ex ṣafərnō.']\n",
    "print(len(sents_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get four different annotation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_scores(annotations, sents_ann):\n",
    "    \n",
    "    all=0\n",
    "    correct=0\n",
    "\n",
    "    scores={\"NOUN\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0}, \n",
    "            \"VERB\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"AUX\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"ADJ\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"ADV\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"PRON\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"NUM\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"SCONJ\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"CCONJ\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"DET\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"PREP\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"PART\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"INTJ\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0},\n",
    "            \"PROPN\": {\"total\": 0, \"corr_lemma\": 0, \"corr_pos\": 0, \"corr_gramm\": 0}}\n",
    "\n",
    "    all=len(annotations)\n",
    "    for an_word in annotations:\n",
    "            \n",
    "        analyses = a.analyze_words(an_word[0])\n",
    "        pos_tag=an_word[2]\n",
    "        scores[pos_tag][\"total\"]+=1\n",
    "\n",
    "        p=True\n",
    "        q=True\n",
    "        r=True\n",
    "\n",
    "        for ana in analyses:\n",
    "            if ana.lemma!=\"\":\n",
    "                    \n",
    "                if p==True:\n",
    "                    if ana.lemma==an_word[1]:\n",
    "                        scores[pos_tag][\"corr_lemma\"]+=1\n",
    "                        p=False\n",
    "                if q==True:\n",
    "                    if ana.gramm.split(\",\")[0]==pos_tag:\n",
    "                        scores[pos_tag][\"corr_pos\"]+=1\n",
    "                        q=False\n",
    "                if r==True:\n",
    "                    if set(ana.gramm.split(\",\")[1:])==an_word[3]:\n",
    "                        scores[pos_tag][\"corr_gramm\"]+=1\n",
    "                        r=False\n",
    "                \n",
    "                if ana.lemma==an_word[1] and ana.gramm.split(\",\")[0]==pos_tag and set(ana.gramm.split(\",\")[1:])==an_word[3]:\n",
    "                    correct+=1\n",
    "    \n",
    "    \n",
    "    print(\"Total correct words: \", correct)\n",
    "    print(\"Total words: \", all)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct words:  938\n",
      "Total words:  1055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NOUN': {'total': 251, 'corr_lemma': 226, 'corr_pos': 227, 'corr_gramm': 225},\n",
       " 'VERB': {'total': 296, 'corr_lemma': 227, 'corr_pos': 228, 'corr_gramm': 222},\n",
       " 'AUX': {'total': 38, 'corr_lemma': 38, 'corr_pos': 38, 'corr_gramm': 38},\n",
       " 'ADJ': {'total': 21, 'corr_lemma': 19, 'corr_pos': 19, 'corr_gramm': 19},\n",
       " 'ADV': {'total': 53, 'corr_lemma': 47, 'corr_pos': 47, 'corr_gramm': 47},\n",
       " 'PRON': {'total': 48, 'corr_lemma': 48, 'corr_pos': 48, 'corr_gramm': 48},\n",
       " 'NUM': {'total': 23, 'corr_lemma': 23, 'corr_pos': 23, 'corr_gramm': 23},\n",
       " 'SCONJ': {'total': 20, 'corr_lemma': 18, 'corr_pos': 18, 'corr_gramm': 18},\n",
       " 'CCONJ': {'total': 83, 'corr_lemma': 83, 'corr_pos': 83, 'corr_gramm': 83},\n",
       " 'DET': {'total': 29, 'corr_lemma': 29, 'corr_pos': 29, 'corr_gramm': 29},\n",
       " 'PREP': {'total': 135, 'corr_lemma': 134, 'corr_pos': 134, 'corr_gramm': 134},\n",
       " 'PART': {'total': 33, 'corr_lemma': 32, 'corr_pos': 32, 'corr_gramm': 32},\n",
       " 'INTJ': {'total': 9, 'corr_lemma': 8, 'corr_pos': 8, 'corr_gramm': 8},\n",
       " 'PROPN': {'total': 16, 'corr_lemma': 12, 'corr_pos': 12, 'corr_gramm': 12}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_annotation_scores(ann1, sents_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct words:  557\n",
      "Total words:  670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NOUN': {'total': 208, 'corr_lemma': 185, 'corr_pos': 186, 'corr_gramm': 184},\n",
       " 'VERB': {'total': 258, 'corr_lemma': 191, 'corr_pos': 192, 'corr_gramm': 186},\n",
       " 'AUX': {'total': 18, 'corr_lemma': 18, 'corr_pos': 18, 'corr_gramm': 18},\n",
       " 'ADJ': {'total': 20, 'corr_lemma': 18, 'corr_pos': 18, 'corr_gramm': 18},\n",
       " 'ADV': {'total': 29, 'corr_lemma': 23, 'corr_pos': 23, 'corr_gramm': 23},\n",
       " 'PRON': {'total': 21, 'corr_lemma': 21, 'corr_pos': 21, 'corr_gramm': 21},\n",
       " 'NUM': {'total': 14, 'corr_lemma': 14, 'corr_pos': 14, 'corr_gramm': 14},\n",
       " 'SCONJ': {'total': 10, 'corr_lemma': 8, 'corr_pos': 8, 'corr_gramm': 8},\n",
       " 'CCONJ': {'total': 2, 'corr_lemma': 2, 'corr_pos': 2, 'corr_gramm': 2},\n",
       " 'DET': {'total': 16, 'corr_lemma': 16, 'corr_pos': 16, 'corr_gramm': 16},\n",
       " 'PREP': {'total': 39, 'corr_lemma': 38, 'corr_pos': 38, 'corr_gramm': 38},\n",
       " 'PART': {'total': 12, 'corr_lemma': 11, 'corr_pos': 11, 'corr_gramm': 11},\n",
       " 'INTJ': {'total': 7, 'corr_lemma': 6, 'corr_pos': 6, 'corr_gramm': 6},\n",
       " 'PROPN': {'total': 16, 'corr_lemma': 12, 'corr_pos': 12, 'corr_gramm': 12}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_annotation_scores(ann2, sents_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct words:  938\n",
      "Total words:  952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NOUN': {'total': 229, 'corr_lemma': 226, 'corr_pos': 227, 'corr_gramm': 225},\n",
       " 'VERB': {'total': 228, 'corr_lemma': 227, 'corr_pos': 228, 'corr_gramm': 222},\n",
       " 'AUX': {'total': 38, 'corr_lemma': 38, 'corr_pos': 38, 'corr_gramm': 38},\n",
       " 'ADJ': {'total': 19, 'corr_lemma': 19, 'corr_pos': 19, 'corr_gramm': 19},\n",
       " 'ADV': {'total': 49, 'corr_lemma': 47, 'corr_pos': 47, 'corr_gramm': 47},\n",
       " 'PRON': {'total': 48, 'corr_lemma': 48, 'corr_pos': 48, 'corr_gramm': 48},\n",
       " 'NUM': {'total': 23, 'corr_lemma': 23, 'corr_pos': 23, 'corr_gramm': 23},\n",
       " 'SCONJ': {'total': 19, 'corr_lemma': 18, 'corr_pos': 18, 'corr_gramm': 18},\n",
       " 'CCONJ': {'total': 83, 'corr_lemma': 83, 'corr_pos': 83, 'corr_gramm': 83},\n",
       " 'DET': {'total': 29, 'corr_lemma': 29, 'corr_pos': 29, 'corr_gramm': 29},\n",
       " 'PREP': {'total': 134, 'corr_lemma': 134, 'corr_pos': 134, 'corr_gramm': 134},\n",
       " 'PART': {'total': 32, 'corr_lemma': 32, 'corr_pos': 32, 'corr_gramm': 32},\n",
       " 'INTJ': {'total': 8, 'corr_lemma': 8, 'corr_pos': 8, 'corr_gramm': 8},\n",
       " 'PROPN': {'total': 13, 'corr_lemma': 12, 'corr_pos': 12, 'corr_gramm': 12}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_annotation_scores(ann3, sents_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct words:  557\n",
      "Total words:  571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NOUN': {'total': 188, 'corr_lemma': 185, 'corr_pos': 186, 'corr_gramm': 184},\n",
       " 'VERB': {'total': 192, 'corr_lemma': 191, 'corr_pos': 192, 'corr_gramm': 186},\n",
       " 'AUX': {'total': 18, 'corr_lemma': 18, 'corr_pos': 18, 'corr_gramm': 18},\n",
       " 'ADJ': {'total': 18, 'corr_lemma': 18, 'corr_pos': 18, 'corr_gramm': 18},\n",
       " 'ADV': {'total': 25, 'corr_lemma': 23, 'corr_pos': 23, 'corr_gramm': 23},\n",
       " 'PRON': {'total': 21, 'corr_lemma': 21, 'corr_pos': 21, 'corr_gramm': 21},\n",
       " 'NUM': {'total': 14, 'corr_lemma': 14, 'corr_pos': 14, 'corr_gramm': 14},\n",
       " 'SCONJ': {'total': 9, 'corr_lemma': 8, 'corr_pos': 8, 'corr_gramm': 8},\n",
       " 'CCONJ': {'total': 2, 'corr_lemma': 2, 'corr_pos': 2, 'corr_gramm': 2},\n",
       " 'DET': {'total': 16, 'corr_lemma': 16, 'corr_pos': 16, 'corr_gramm': 16},\n",
       " 'PREP': {'total': 38, 'corr_lemma': 38, 'corr_pos': 38, 'corr_gramm': 38},\n",
       " 'PART': {'total': 11, 'corr_lemma': 11, 'corr_pos': 11, 'corr_gramm': 11},\n",
       " 'INTJ': {'total': 6, 'corr_lemma': 6, 'corr_pos': 6, 'corr_gramm': 6},\n",
       " 'PROPN': {'total': 13, 'corr_lemma': 12, 'corr_pos': 12, 'corr_gramm': 12}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_annotation_scores(ann4, sents_ann)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
