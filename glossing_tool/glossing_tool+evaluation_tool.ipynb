{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uniparser_morph in /home/sofia1236/.local/lib/python3.8/site-packages (2.7.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install uniparser_morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uniparser_morph import Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Analyzer()\n",
    "a.load_grammar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossing tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clitics=[\"b\", \"bə\", \"əb\", \"p\", \"əp\", \"f\", \"əf\", \"bib\", \"bil\", \"m\", \"ʕa\", \"ʕal\", \"aʕl\", \"ʕl\", \n",
    "         \"ʕam\", \"ʕamma\", \"l\", \"mn\", \"mnə\", \"əm\", \"əmn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glossing tool is a function: it takes sentences as a string and returns this text glossed: there is morphological segmentation in first line and morphological annotation in second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloss_text(text_to_gloss):\n",
    "\n",
    "    sents_to_gloss=\"\"\n",
    "    f=open(text_to_gloss, \"r\", encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        sents_to_gloss+=line[:-1]+\" \"\n",
    "    f.close()\n",
    "\n",
    "    final_tokens=[]\n",
    "    sents=nltk.sent_tokenize(sents_to_gloss)\n",
    "\n",
    "    for sent in sents:\n",
    "        first_tokens = nltk.word_tokenize(sent)\n",
    "\n",
    "        help_tokens=[]\n",
    "        for tok in first_tokens:\n",
    "            tok=tok.lower()\n",
    "            if \"-\" in tok:\n",
    "                help_tokens.extend(tok.split(\"-\"))\n",
    "            elif \"_\" in tok:\n",
    "                help_tokens.extend(tok.split(\"_\"))\n",
    "            elif tok.isalpha() or \"ḏ̣\" in tok:\n",
    "                help_tokens.append(tok)\n",
    "        \n",
    "        final_tokens.append(help_tokens)\n",
    "    \n",
    "\n",
    "    words=[]\n",
    "    sep_words=[]\n",
    "    glossed_words=[]\n",
    "\n",
    "    for sent in final_tokens:\n",
    "        help_words=[]\n",
    "        help_sep_words=[]\n",
    "        help_glossed_words=[]\n",
    "\n",
    "        analyses = a.analyze_words(sent)\n",
    "\n",
    "        for ana in analyses:\n",
    "            help_words.append(ana[0].wf)\n",
    "            \n",
    "            if ana[0].lemma==\"\":\n",
    "                 help_sep_words.append(\"UNK\")\n",
    "                 help_glossed_words.append(\"UNK\")\n",
    "\n",
    "            if ana[0].wfGlossedStd!=\"\":\n",
    "                if \"<\" in ana[0].wfGlossedStd!=\"\":\n",
    "                    ana_std = \"\"\n",
    "                    for symb in ana[0].wfGlossedStd:\n",
    "                        if symb!= \"<\" and symb!=\">\":\n",
    "                            ana_std+=symb\n",
    "                    help_sep_words.append(ana_std)\n",
    "                else:        \n",
    "                    help_sep_words.append(ana[0].wfGlossedStd)\n",
    "            elif ana[0].wfGlossed!=\"\":\n",
    "                help_sep_words.append(ana[0].wfGlossed)\n",
    "                \n",
    "            if \"Praet\" in ana[0].gramm:\n",
    "                if \"-\" in ana[0].gloss:\n",
    "                    ana_splitted=ana[0].gloss.split(\"-\")\n",
    "                else:\n",
    "                    ana_splitted=ana[0].gloss.split(\"=\")\n",
    "                if len(ana_splitted)==1:\n",
    "                    an=ana_splitted[0]+\".PST\"\n",
    "                    help_glossed_words.append(an)\n",
    "                else:\n",
    "                    an=ana_splitted[0]+\".PST-\"+\"-\".join(ana_splitted[1:])\n",
    "                    an=an.strip(\"-\")\n",
    "                    help_glossed_words.append(an)\n",
    "            elif \"Subj\" in ana[0].gramm:\n",
    "                if \"-\" in ana[0].gloss:\n",
    "                    ana_splitted=ana[0].gloss.split(\"-\")\n",
    "                else:\n",
    "                    ana_splitted=ana[0].gloss.split(\"=\")\n",
    "                if len(ana_splitted)==1:\n",
    "                    an=ana_splitted[0]+\".SBJV\"\n",
    "                    help_glossed_words.append(an)\n",
    "                elif ana_splitted[0]==\"STEM\":\n",
    "                    an=ana_splitted[0]+\".SBJV-\"+\"-\".join(ana_splitted[1:])\n",
    "                    an=an.strip(\"-\")\n",
    "                    help_glossed_words.append(an)\n",
    "                else:\n",
    "                    an=ana_splitted[0]+\"-\"+ana_splitted[1]+\".SBJV-\"+\"-\".join(ana_splitted[2:])\n",
    "                    an=an.strip(\"-\")\n",
    "                    help_glossed_words.append(an)\n",
    "            elif \"Imp\" in ana[0].gramm:\n",
    "                if \"-\" in ana[0].gloss:\n",
    "                    ana_splitted=ana[0].gloss.split(\"-\")\n",
    "                else:\n",
    "                    ana_splitted=ana[0].gloss.split(\"=\")\n",
    "                if len(ana_splitted)==1:\n",
    "                    an=ana_splitted[0]+\".IPV\"\n",
    "                    help_glossed_words.append(an)\n",
    "                else:\n",
    "                    an=ana_splitted[0]+\".IPV-\"+\"-\".join(ana_splitted[1:])\n",
    "                    an=an.strip(\"-\")\n",
    "                    help_glossed_words.append(an)\n",
    "            elif \"Prs\" in ana[0].gramm:\n",
    "                if \"-\" in ana[0].gloss:\n",
    "                    ana_splitted=ana[0].gloss.split(\"-\")\n",
    "                else:\n",
    "                    ana_splitted=ana[0].gloss.split(\"=\")\n",
    "                if len(ana_splitted)==1:\n",
    "                    an=ana_splitted[0]+\".PRS\"\n",
    "                    help_glossed_words.append(an)\n",
    "                elif ana_splitted[0]==\"STEM\":\n",
    "                    an=ana_splitted[0]+\".PRS-\"+\"-\".join(ana_splitted[1:])\n",
    "                    an=an.strip(\"-\")\n",
    "                    help_glossed_words.append(an)\n",
    "                else:\n",
    "                    an=ana_splitted[0]+\"-\"+ana_splitted[1]+\".PRS-\"+\"-\".join(ana_splitted[2:])\n",
    "                    an=an.strip(\"-\")\n",
    "                    help_glossed_words.append(an)\n",
    "            elif \"Perf\" in ana[0].gramm:\n",
    "                if \"-\" in ana[0].gloss:\n",
    "                    ana_splitted=ana[0].gloss.split(\"-\")\n",
    "                else:\n",
    "                    ana_splitted=ana[0].gloss.split(\"=\")\n",
    "                if len(ana_splitted)==1:\n",
    "                    an=ana_splitted[0]+\".PRF\"\n",
    "                    help_glossed_words.append(an)\n",
    "                elif ana_splitted[0]==\"STEM\":\n",
    "                    an=ana_splitted[0]+\".PRF-\"+\"-\".join(ana_splitted[1:])\n",
    "                    an=an.strip(\"-\")\n",
    "                    help_glossed_words.append(an)\n",
    "                else:\n",
    "                    an=ana_splitted[0]+\"-\"+ana_splitted[1]+\".PRF-\"+\"-\".join(ana_splitted[2:])\n",
    "                    an=an.strip(\"-\")\n",
    "                    help_glossed_words.append(an)\n",
    "\n",
    "            elif \"ZPl\" in ana[0].gramm and \"NUM\" not in ana[0].gloss:\n",
    "                if \"-\" in ana[0].gloss:\n",
    "                    ana_splitted=ana[0].gloss.split(\"-\")\n",
    "                else:\n",
    "                    ana_splitted=ana[0].gloss.split(\"=\")\n",
    "                an=ana_splitted[0]+\".NUM\"\n",
    "                help_glossed_words.append(an)\n",
    "            elif \"PROPN\" in ana[0].gramm:\n",
    "                help_glossed_words.append(\"PN\")\n",
    "\n",
    "            elif ana[0].gloss!=\"\":\n",
    "                help_glossed_words.append(ana[0].gloss)\n",
    "    \n",
    "        words.append(help_words)\n",
    "        sep_words.append(help_sep_words)\n",
    "        glossed_words.append(help_glossed_words)\n",
    "\n",
    "\n",
    "    #correction with \"=\" signs\n",
    "    new_sep_words=[]\n",
    "\n",
    "    for q in range(len(glossed_words)):\n",
    "        glossed_sent=[]\n",
    "        for p in range(len(glossed_words[q])):\n",
    "            if \"=\" in glossed_words[q][p]:\n",
    "                glossed_parts=sep_words[q][p].split(\"-\")\n",
    "                new_sep_word=\"-\".join(glossed_parts[:-1])+\"=\"+glossed_parts[-1]\n",
    "                glossed_sent.append(new_sep_word)\n",
    "            else:\n",
    "                glossed_sent.append(sep_words[q][p])\n",
    "        new_sep_words.append(glossed_sent)\n",
    "\n",
    "    #merge words with clitics\n",
    "    new_new_sep_words=[]\n",
    "    new_glossed_words=[]\n",
    "\n",
    "    for q in range(len(new_sep_words)):\n",
    "        new_new_sep_sent=[]\n",
    "        new_glossed_sent=[]\n",
    "        clitic1=\"\"\n",
    "        clitic2=\"\"\n",
    "\n",
    "        for p in range(len(new_sep_words[q])):\n",
    "            \n",
    "            if clitic1 != \"\":\n",
    "                to_append1=clitic1+\"=\"+new_sep_words[q][p]\n",
    "                to_append2=clitic2+\"=\"+glossed_words[q][p]\n",
    "                new_new_sep_sent.append(to_append1)\n",
    "                new_glossed_sent.append(to_append2)\n",
    "                clitic1=\"\"\n",
    "                clitic2=\"\"\n",
    "            elif new_sep_words[q][p] in clitics:\n",
    "                clitic1=new_sep_words[q][p]\n",
    "                clitic2=glossed_words[q][p]  \n",
    "            else:\n",
    "                new_new_sep_sent.append(new_sep_words[q][p])\n",
    "                new_glossed_sent.append(glossed_words[q][p])\n",
    "        \n",
    "        new_new_sep_words.append(new_new_sep_sent)\n",
    "        new_glossed_words.append(new_glossed_sent)\n",
    "\n",
    "\n",
    "    f=open(\"glosses_output.txt\", \"w\", encoding=\"utf-8\")\n",
    "    for i in range(len(words)):\n",
    "        f.write(\"\\t\".join(new_new_sep_words[i]))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\t\".join(new_glossed_words[i]))\n",
    "        f.write(\"\\n\\n\")\n",
    "    f.close()\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        print(\"\\t\".join(words[i]))\n",
    "        print(\"\\t\".join(new_new_sep_words[i]))\n",
    "        print(\"\\t\".join(new_glossed_words[i]))\n",
    "        print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the output looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orḥa\tnībin\tnizʕūrin\tʕumraynaḥ\tyaʕni\tmett\tarpaʕʕasər\tišən\n",
      "orḥ-a\tn-īb-in\tn-izʕūr-in\tʕumr-ay=naḥ\tyaʕni\tmett\tarpaʕʕasər\tišən\n",
      "STEM-FREE\t1-be-MP\t1-STEM-MP.INDF\tSTEM-P=1P\tHES\tSTEM\tfourteen.F\tSTEM\n",
      "\n",
      "silḳiṯ\tana\tfēris\tw\taḥḥaḏ\tušme\tʕazīz\tšōʕra\tʕa\tʕarḳūba\tp\tḥaṣṣil\tpayṯaḥ\n",
      "silḳ-iṯ\tana\tfēris\tw\taḥḥaḏ\tušm=e\tʕazīz\tšōʕr-a\tʕa=ʕarḳūb-a\tb=ḥaṣṣil\tpayṯ=aḥ\n",
      "STEM.PST-1S\tI\tPN\tand\tone.M\tSTEM=3ms\tPN\tSTEM-FREE\tto=STEM-FREE\tin=STEM.IPV\tSTEM=1P\n",
      "\n",
      "silḳinnaḥ\tl\telʕel\tḥminnaḥ\txēfa\trabb\twazne\tnōfeḳ\tmett\tṯarč\temʕa\tkīlo\tṯaḳḳen\tex\tṭōpṯa\tmtawwar\tmtaḥpal\tyaʕni\n",
      "silḳ-innaḥ\tl=elʕel\tḥm-innaḥ\txēf-a\trabb\twazn=e\tnōfeḳ\tmett\tṯarč\temʕa\tUNK\tṯaḳḳen\tex\tṭōp-ṯ-a\tmtawwar\tUNK\tyaʕni\n",
      "STEM.PST-1P\tto=above\tSTEM.PST-1P\tSTEM-FREE\tSTEM\tSTEM=3ms\tSTEM.PRS\tSTEM\ttwo.F\thundred\tUNK\tSTEM.PRF\tlike\tSTEM-F-FREE\tSTEM.PRS\tUNK\tHES\n",
      "\n",
      "amrille\tṯāx\tntaḥəklell\tlanna\txēfa\tw\tču\tnyaḏḏīʕin\tmō\tbatte\tyinfuḳ\tm\ttaḥkīll\tlanna\txēfa\tw\terraʕ\tminnaynaḥ\tōṯ\tpayṯyōṯa\tbaḥar\n",
      "amr-il=le\tṯā-x\tUNK\thanna\txēf-a\tw\tču\tn-yaḏḏīʕ-in\tmō\tbatt=e\tUNK\tm=taḥkīl-l\thanna\txēf-a\tw\terraʕ\tminnay=naḥ\tōṯ\tpayṯ-yōṯ-a\tbaḥar\n",
      "STEM.PST-1S=3ms.IO\tSTEM.IPV-MS\tUNK\tthis.M\tSTEM-FREE\tand\tNEG\t1-STEM.PRF-MP\twhat\tFUT=3ms\tUNK\tfrom=STEM.PRF-DOM\tthis.M\tSTEM-FREE\tand\tunder\tfrom=1P\tEXIST\tSTEM-P-FREE\tmuch\n",
      "\n",
      "ōmar\tē\ttafəšlaḥəl\tlanna\txēfa\tinḥeč\tʕa\ttaḥkel\tb\tanna\tʕarḳūba\tmasōfča\tmett\tṯarč\teṯlaṯ\temʕa\tmičər\n",
      "ōmar\tē\ttafəš-laḥəl\thanna\txēf-a\tinḥeč\tʕa=taḥkel\tb=hanna\tʕarḳūb-a\tmasōf-č-a\tmett\tṯarč\teṯlaṯ\temʕa\tmičər\n",
      "STEM.PST\tyes\tSTEM.PST-1P\tthis.M\tSTEM-FREE\tSTEM.PST\tto=STEM.PRF\tin=this.M\tSTEM-FREE\tSTEM-F-FREE\tSTEM\ttwo.F\tthree.F\thundred\tSTEM.NUM\n",
      "\n",
      "iḥbaṭ\tʕa\tpayṯa\tl\tšaxṣa\tušme\tʕali\tḥammūd\thanna\tm\trappōylə\tblōta\tyaʕni\n",
      "iḥbaṭ\tʕa=payṯ-a\tl=šaxṣ-a\tušm=e\tUNK\tḥammūd\thanna\tm=UNK\tblōt-a\tyaʕni\n",
      "STEM.PST\tto=STEM-FREE\tto=STEM-FREE\tSTEM=3ms\tUNK\tPN\tthis.M\tfrom=UNK\tSTEM-FREE\tHES\n",
      "\n",
      "waybin\tžmīʕin\tġappe\tommṯa\tbaḥar\takṯar\tmn\teʕsar\tḥammešʕasər\tzalman\n",
      "w-ayb-in\tžmīʕ-in\tġapp=e\tomm-ṯ-a\tbaḥar\takṯar\tmn=eʕsar\tḥammešʕasər\tzalm-an\n",
      "PST-be-MP\tSTEM.PRF-MP\twith=3ms\tSTEM-F-FREE\tmuch\tSTEM\tfrom=ten.F\tfifteen.F\tSTEM-NUM.F\n",
      "\n",
      "ḏukkil\tisḳaṭ\thanna\txēfa\tražžaṯ\tblōta\txulla\tsawa\tliʔannu\tawwal\tmett\timḥaḳ\tʕakkōrəš\tšimenṭo\tbaʕdēn\tinḥeč\tʕa\tʕakkōrlə\txšūra\tčbarle\tšeṯṯ\tešbaʕ\txšurīyan\n",
      "ḏukkil\tisḳaṭ\thanna\txēf-a\tUNK\tblōt-a\txull=a\tsawa\tliʔannu\tawwal\tmett\tUNK\tʕakkōr=l\tšimenṭo\tbaʕdēn\tinḥeč\tʕa=ʕakkōr=l\txšūr-a\tčbar-le\tšeṯṯ\tešbaʕ\txšurī-yan\n",
      "when\tSTEM.PST\tthis.M\tSTEM-FREE\tUNK\tSTEM-FREE\tSTEM=3fs\tSTEM\tbecause\tfirst\tSTEM\tUNK\tSTEM=HD\tSTEM\tthen\tSTEM.PST\tto=STEM=HD\tSTEM-FREE\tSTEM.PST-3ms.IO\tsix.F\tseven.F\tSTEM-NUM.F\n",
      "\n",
      "ḏukkil\tiḥbaṭ\tʕa\tpayṯe\tinḥeč\thanna\tʕafra\tʕlayy\tw\tḳōmaṯ\thōḏ\tġabrṯa\telġul\tlorkaʕ\tbaḳḳar\tex\tbattayy\tynufḳun\tm\tṯarʕa\n",
      "ḏukkil\tiḥbaṭ\tʕa=payṯ=e\tinḥeč\thanna\tʕafr-a\tʕlay=y\tw\tḳōm-aṯ\thōḏ\tUNK\telġul\tlorkaʕ\tbaḳḳar\tex\tbattay=y\ty-nufḳ-un\tm=ṯarʕ-a\n",
      "when\tSTEM.PST\tto=STEM=3ms\tSTEM.PST\tthis.M\tSTEM-FREE\tto=3P\tand\tSTEM.PST-3fs\tthis.F\tUNK\tSTEM\tbut.not\tSTEM.PST\tlike\tFUT=3fp\t3-STEM.SBJV-MP\tfrom=STEM-FREE\n",
      "\n",
      "ana\tḏukkil\tiḥmiṯ\txann\tiṯḳen\tʕimmaynaḥ\tšamṭiṯ\tb\tanna\tʕarḳūba\n",
      "ana\tḏukkil\tiḥm-iṯ\txann\tiṯḳen\tʕimmay=naḥ\tšamṭ-iṯ\tb=hanna\tʕarḳūb-a\n",
      "I\twhen\tSTEM.PST-1S\tlike.so\tSTEM.PST\twith=1P\tSTEM.PST-1S\tin=this.M\tSTEM-FREE\n",
      "\n",
      "ōṯ\tšīra\tṭamriṯ\terraʕ\tmenne\n",
      "ōṯ\tšīr-a\tṭamr-iṯ\terraʕ\tmenn=e\n",
      "EXIST\tSTEM-FREE\tSTEM.PST-1S\tunder\tfrom=3ms\n",
      "\n",
      "ʕazīz\tšōʕra\tla\taḳtar\tyišmuṭ\tōčem\tb\tʕarḳūba\n",
      "ʕazīz\tšōʕr-a\tla\taḳtar\tyi-šmuṭ\tōčem\tb=ʕarḳūb-a\n",
      "PN\tSTEM-FREE\tNEG\tSTEM.PST\t3M-STEM.SBJV\tSTEM.PST\tin=STEM-FREE\n",
      "\n",
      "isleḳ\tfōzi\tw\tḥammūd\tbnōyəl\tʕali\tyiḥmun\tmō\tiṯḳen\tbōṯar\tmett\trobʕiš\tšaʕṯa\n",
      "isleḳ\tUNK\tw\tḥammūd\tbn-ōy=l\tUNK\tyi-ḥm-un\tmō\tiṯḳen\tbōṯar\tmett\trobʕ=l\tšaʕ-ṯ-a\n",
      "STEM.PST\tUNK\tand\tPN\tSTEM-P=HD\tUNK\t3-STEM.SBJV-MP\twhat\tSTEM.PST\tSTEM\tSTEM\tSTEM=HD\tSTEM-F-FREE\n",
      "\n",
      "isleḳ\tʕa\tʕarḳūba\tḥmull\tʕazīz\tlaḳṭunne\n",
      "isleḳ\tʕa=ʕarḳūb-a\tUNK\tʕazīz\tlaḳṭ-un-n=e\n",
      "STEM.PST\tto=STEM-FREE\tUNK\tPN\tSTEM.PST-3mp-PLEO=3ms\n",
      "\n",
      "ʕazīz\tm\tzawʕe\tmiskīna\tšayšar\tbə\tbrōḳe\n",
      "ʕazīz\tm=zawʕ=e\tmiskīn-a\tšayšar\tb=brōḳ=e\n",
      "PN\tfrom=STEM=3ms\tSTEM-FREE\tSTEM.IPV\tin=STEM=3ms\n",
      "\n",
      "amellun\tw\tḥayyil\talō\tčūb\tana\thanna\tfēris\tti\ttaḥəklil\txēfa\n",
      "amel-lun\tw\tUNK\talō\tčūb\tana\thanna\tfēris\tti\tUNK\txēf-a\n",
      "STEM.PST-3mp.IO\tand\tUNK\tGod\tSTEM.IPV\tI\tthis.M\tPN\tREL\tUNK\tSTEM-FREE\n",
      "\n",
      "iṯḳen\tmtawwrin\taʕəl\tana\tniṭmer\tla\tsčahət\taʕəl\n",
      "iṯḳen\tmtawwr-in\taʕəl\tana\tni-ṭmer\tla\tUNK\taʕəl\n",
      "STEM.PST\tSTEM.PRS-MP\tto\tI\t1-STEM.PRF\tNEG\tUNK\tto\n",
      "\n",
      "ʕirpaṯ\tšimša\tw\tana\tnḳayyam\telʕel\n",
      "UNK\tšimš-a\tw\tana\tUNK\telʕel\n",
      "UNK\tSTEM-FREE\tand\tI\tUNK\tabove\n",
      "\n",
      "eppay\tšattar\tommṯa\tytawwrun\taʕəl\tazaʕ\taʕəl\tliʔannu\tana\tnwaḥtōnay\tnōb\tġappe\tp\tpayṯa\n",
      "eppay\tšattar\tomm-ṯ-a\ty-tawwr-un\taʕəl\tazaʕ\taʕəl\tliʔannu\tana\tUNK\tn-ōb\tġapp=e\tb=payṯ-a\n",
      "STEM\tSTEM.IPV\tSTEM-F-FREE\t3-STEM.SBJV-MP\tto\tSTEM.PST\tto\tbecause\tI\tUNK\t1-be\twith=3ms\tin=STEM-FREE\n",
      "\n",
      "ḏukkil\tmassaṯ\ttunya\tkayyes\tana\tniḥčiṯ\tm\tʕarḳūba\n",
      "ḏukkil\tUNK\ttuny-a\tkayyes\tana\tniḥč-iṯ\tm=ʕarḳūb-a\n",
      "when\tUNK\tSTEM-FREE\tSTEM\tI\tSTEM.PST-1S\tfrom=STEM-FREE\n",
      "\n",
      "niḥčiṯ\tʕa\tpayṯa\tʕayniṯ\tm\tšuppōka\txann\tḥmiččil\teppay\tġappe\tommṯa\tw\tʕam\tsapseb\taʕəl\n",
      "niḥč-iṯ\tʕa=payṯ-a\tʕayn-iṯ\tm=šuppōk-a\txann\tḥm-ič-č=l\teppay\tġapp=e\tomm-ṯ-a\tw\tʕam=UNK\taʕəl\n",
      "STEM.PST-1S\tto=STEM-FREE\tSTEM.PST-1S\tfrom=STEM-FREE\tlike.so\tSTEM.PST-1S-PLEO=DOM\tSTEM\twith=3ms\tSTEM-F-FREE\tand\tPP=UNK\tto\n",
      "\n",
      "ḥakiččil\temmay\tm\tšuppōka\temmay\tḳaʕya\tʕam\tbōxya\n",
      "UNK\temmay\tm=šuppōk-a\temmay\tḳaʕy-a\tʕam=UNK\n",
      "UNK\tSTEM\tfrom=STEM-FREE\tSTEM\tSTEM.PRF-F\tPP=UNK\n",
      "\n",
      "amrilla\tṯill\tana\n",
      "amril-la\tṯ-ill\tana\n",
      "STEM.PRS-3fs.IO\tSTEM.PST-1S\tI\n",
      "\n",
      "ōmar\thanik\tčōb\tya\tčḳubrinni\tʕbār\n",
      "ōmar\thanik\tč-ōb\tya\tč-ḳubr-inn=i\tʕbār\n",
      "STEM.PST\tSTEM\t2-be\tSTEM\t2-STEM.SBJV-PLEO=1S\tSTEM.IPV\n",
      "\n",
      "amrilla\tḳaṭill\teppay\n",
      "amril-la\tUNK\teppay\n",
      "STEM.PRS-3fs.IO\tUNK\tSTEM\n",
      "\n",
      "ōmar\tlā\tʕbār\n",
      "ōmar\tlā\tʕbār\n",
      "STEM.PST\tNEG\tSTEM.IPV\n",
      "\n",
      "m\tʕabərṯi\ttuġray\tnawīll\tb\tanna\tkaffa\tw\tm\tmaləḳṭa\n",
      "m=ʕabər-ṯ=i\ttuġray\tnawīl-l\tb=hanna\tkaff-a\tw\tm=maləḳṭ-a\n",
      "from=STEM-F=1S\tSTEM\tSTEM.PRF-1S\tin=this.M\tSTEM-FREE\tand\tfrom=STEM-FREE\n",
      "\n",
      "la\twkʕi\tbaḥar\tbess\tana\tmuxrōmča\tnxallṣell\tbaʕḏ̣\tšwiččil\tḥōl\tʕanbōx\tw\ttuġray\tzlill\tʕa\tfarəšṯa\n",
      "la\tUNK\tbaḥar\tbess\tana\tmuxrōmča\tUNK\tbaʕḏ̣\tšw-ič-č=l\tḥōl\tUNK\tw\ttuġray\tzl-ill\tʕa=farəš-ṯ-a\n",
      "NEG\tUNK\tmuch\tSTEM\tI\tbecause\tUNK\tSTEM\tSTEM.PST-1S-PLEO=DOM\tSTEM\tUNK\tand\tSTEM\tSTEM.PST-1S\tto=STEM-F-FREE\n",
      "\n",
      "atar\thann\təġmōʕča\tstiḳō\tanaḥ\tw\thinn\tyaʕni\tw\tšbabaynaḥ\n",
      "atar\thann\tUNK\tstiḳ-ō\tanaḥ\tw\thinn\tyaʕni\tw\tšbab-ay=naḥ\n",
      "so\tthese\tUNK\tSTEM-P\twe\tand\tthey\tHES\tand\tSTEM-P=1P\n",
      "\n",
      "amrull\teppay\tṭálama\tšaġəlṯa\tsalimōy\tw\thanna\tču\tbattaynaḥ\tmett\n",
      "amr-ul=l\teppay\tṭálam-a\tšaġəl-ṯ-a\tsalim-ōy\tw\thanna\tču\tbattay=naḥ\tmett\n",
      "STEM.PST-3mp=3mp.IO\tSTEM\tSTEM-FREE\tSTEM-F-FREE\tSTEM-MP.DEF\tand\tthis.M\tNEG\tFUT=1P\tSTEM\n",
      "\n",
      "bess\tamellun\teppay\tmō\tčičḏ̣arrīrin\tanaḥ\tntafʕille\n",
      "bess\tamel-lun\teppay\tmō\tUNK\tanaḥ\tn-tafʕ-il=le\n",
      "STEM\tSTEM.PST-3mp.IO\tSTEM\twhat\tUNK\twe\t1-STEM.PRS-MP=3ms\n",
      "\n",
      "mappēlun\txšurō\tmṣallḥill\tpayṯun\tw\tmallex\tḥōla\n",
      "UNK\txšur-ō\tmṣallḥ-il=l\tpayṯ=un\tw\tmallex\tḥōl-a\n",
      "UNK\tSTEM-P\tSTEM.PRS-MP=1S\tSTEM=3mp\tand\tSTEM.PRS\tSTEM-FREE\n",
      "\n",
      "atar\tana\tm\tžokər\tmn\tanna\tʕazīz\tḳōmiṯ\tassḳiččil\tḥōne\tbōṯar\tiṯər\tṯlōṯa\tyūm\tʕa\tʕarḳūba\tlēle\tw\tl\taḥḥaḏ\tušme\tfawwāz\tfawwāz\tḥalāl\n",
      "atar\tana\tm=UNK\tmn=hanna\tʕazīz\tḳōm-iṯ\ta-ssḳ-ič-č=l\tḥōn=e\tbōṯar\tiṯər\tṯlōṯa\tyūm\tʕa=ʕarḳūb-a\tlē=le\tw\tl=aḥḥaḏ\tušm=e\tUNK\tUNK\tḥalāl\n",
      "so\tI\tfrom=UNK\tfrom=this.M\tPN\tSTEM.PST-1S\tSTEM.PST-1S-PLEO=DOM\tSTEM=3ms\tSTEM\ttwo.M\tthree.M\tSTEM.NUM\tto=STEM-FREE\tto=3ms\tand\tto=one.M\tSTEM=3ms\tUNK\tUNK\tPN\n",
      "\n",
      "amrillun\tbattaḥ\tnišw\tʕēḏəl\tʕēṣ\tṣlība\n",
      "amr-il=lun\tbatt=aḥ\tn-išw\tʕēḏ=l\tUNK\tṣlīb-a\n",
      "STEM.PST-1S=3mp.IO\tFUT=1P\t1-STEM.SBJV\tSTEM=HD\tUNK\tSTEM-FREE\n",
      "\n",
      "hōḏ\tb\tʔāb\tiḳḏum\tm\tʕēḏəl\tʕēṣ\tṣlība\n",
      "hōḏ\tb=UNK\tiḳḏum\tm=ʕēḏ=l\tUNK\tṣlīb-a\n",
      "this.F\tin=UNK\tbefore\tfrom=STEM=HD\tUNK\tSTEM-FREE\n",
      "\n",
      "ōṯ\tʕeššil\tʕurəʕrō\tsummūḳin\terraʕ\tm\tsīḥa\tabəʕḏiṯ\tmiʕlayy\tḳalles\tamrillun\tḳulʕōn\thanna\tsīḥa\n",
      "ōṯ\tUNK\tUNK\tsummūḳ-in\terraʕ\tm=sīḥ-a\tabəʕḏ-iṯ\tmiʕlay=y\tḳalles\tamr-il=lun\tḳulʕ-ōn\thanna\tsīḥ-a\n",
      "EXIST\tUNK\tUNK\tSTEM-MP.INDF\tunder\tfrom=STEM.PRF-F\tSTEM.PST-1S\tSTEM=3P\tSTEM\tSTEM.PST-1S=3mp.IO\tSTEM.IPV-IPV.MP\tthis.M\tSTEM.PRF-F\n",
      "\n",
      "zallun\tyḳulʕuss\tsīḥa\tbess\tmḥunne\thwōyṯa\tōfez\tʕurəʕrō\tsummūḳin\tbə\tffayy\n",
      "zal-lun\tUNK\tsīḥ-a\tbess\tmḥ-un-n=e\thwōy-ṯ-a\tōfez\tUNK\tsummūḳ-in\tb=ffay=y\n",
      "STEM.PST-3mp\tUNK\tSTEM.PRF-F\tSTEM\tSTEM.PST-3mp-PLEO=3ms\tSTEM-F-FREE\tSTEM.PRS\tUNK\tSTEM-MP.INDF\tin=STEM=3P\n",
      "\n",
      "fawwāz\tḳarṭe\tmett\tiṯər\tṯlōṯa\tʕurəʕri\thōṯe\tʕazīz\tḳarṭunne\tmett\tḥiməš\tšičč\tʕurəʕri\tw\tmōmrin\tinnu\tʕurʕra\tsummōḳa\tkōn\tarpʕa\tḥamša\tḳarṭill\taḥḥaḏ\tmōyeṯ\n",
      "UNK\tḳarṭ-e\tmett\tiṯər\tṯlōṯa\tUNK\thōṯe\tʕazīz\tḳarṭ-un-n=e\tmett\tḥiməš\tšičč\tUNK\tw\tmōmr-in\tinnu\tUNK\tsummōḳ-a\tkōn\tarpʕa\tḥamša\tḳarṭ-il=l\taḥḥaḏ\tmōyeṯ\n",
      "UNK\tSTEM.PST-3ms\tSTEM\ttwo.M\tthree.M\tUNK\tthat.M\tPN\tSTEM.PST-3mp-PLEO=3ms\tSTEM\tfifty\tsixty\tUNK\tand\tSTEM.PRS-MP\tSTEM\tUNK\tSTEM-MS.DEF\tSTEM\tfour.M\tfive.M\tSTEM.PRS-MP=1S\tone.M\tSTEM.PRS\n",
      "\n",
      "ana\tnimʕayn\taʕle\tnmiščaḥlə\tffōye\txullun\tʕurəʕrō\ttappīšin\tʕa\tffōye\tw\tballeš\təṣyūḥa\n",
      "ana\tUNK\taʕl=e\tUNK\tff-ōy=e\txull=un\tUNK\tUNK\tʕa=ff-ōy=e\tw\tballeš\tUNK\n",
      "I\tUNK\tto=3ms\tUNK\tSTEM-P=3ms\tSTEM=3mp\tUNK\tUNK\tto=STEM-P=3ms\tand\tSTEM.PST\tUNK\n",
      "\n",
      "batte\tyizʕuḳ\tyā\tʕaḏra\tlorkaʕ\tinfeḳ\tḥesse\n",
      "batt=e\tyi-zʕuḳ\tyā\tUNK\tlorkaʕ\tinfeḳ\tḥess=e\n",
      "FUT=3ms\t3M-STEM.SBJV\tSTEM\tUNK\tbut.not\tSTEM.PST\tSTEM=3ms\n",
      "\n",
      "ana\ttaššriččun\tw\tšamṭiṯ\n",
      "ana\tUNK\tw\tšamṭ-iṯ\n",
      "I\tUNK\tand\tSTEM.PST-1S\n",
      "\n",
      "fawwāz\tinḥeč\tʕa\tblōta\tamell\ttiḏōye\tamellun\tebərxun\tḳarṭunne\tʕurəʕrō\tču\tḳattar\tyallex\tōb\tb\tʕarḳūba\n",
      "UNK\tinḥeč\tʕa=blōt-a\tamel-l\ttiḏ-ōy=e\tamel-lun\tUNK\tḳarṭ-un-n=e\tUNK\tču\tḳattar\ty-allex\tōb\tb=ʕarḳūb-a\n",
      "UNK\tSTEM.PST\tto=STEM-FREE\tSTEM.PST-1S.IO\tSTEM-P=3ms\tSTEM.PST-3mp.IO\tUNK\tSTEM.PST-3mp-PLEO=3ms\tUNK\tNEG\tSTEM.PST\t3M-STEM.SBJV\tSTEM.NUM\tin=STEM-FREE\n",
      "\n",
      "isleḳ\tṭaʕnunne\tw\taḥḥčunne\tinfeḥ\tex\tṭabla\n",
      "isleḳ\tṭaʕn-un-n=e\tw\taḥḥč-un-n=e\tinfeḥ\tex\tṭabl-a\n",
      "STEM.PST\tSTEM.PST-3mp-PLEO=3ms\tand\tSTEM.PST-3mp-PLEO=3ms\tSTEM.PRF\tlike\tSTEM-FREE\n",
      "\n",
      "ḏnōye\tuxxul\teḥḏa\tṯiḳnaṯ\tiṯər\tfišək\tffōye\tču\tbayyen\tkesmun\tmō\thinn\n",
      "ḏn-ōy=e\tuxxul\teḥḏa\tṯiḳn-aṯ\tiṯər\tfišək\tff-ōy=e\tču\tbayyen\tUNK\tmō\thinn\n",
      "STEM-P=3ms\tSTEM\tone.F\tSTEM.PST-3fs\ttwo.M\tSTEM.NUM\tSTEM-P=3ms\tNEG\tSTEM.PST\tUNK\twhat\tthey\n",
      "\n",
      "ġappaynaḥ\tḥkīma\tdaktōr\tnuṣralla\twōb\tḳayya\tčū\tizel\tʕa\tfrānsa\tyičxarraž\tḳayyam\thōxa\tmawžut\tbə\tblōta\tyaʕni\n",
      "ġappay=naḥ\tḥkīm-a\tUNK\tnuṣralla\tw-ōb\tḳayya\tčū\tizel\tʕa=frānsa\tUNK\tḳayyam\thōxa\tmawžut\tb=blōt-a\tyaʕni\n",
      "with=1P\tSTEM-FREE\tUNK\tPN\t3ms-be\tSTEM\tNEG\tSTEM.PRF\tto=PN\tUNK\tSTEM\there\tSTEM\tin=STEM-FREE\tHES\n",
      "\n",
      "zallun\tayṯlulle\tḥiməl\tlanna\tpsōna\txann\tamellun\thanna\tmō\tčūle\ttwō\tbess\tnōz\tnawṣefəlxun\twaṣəfṯa\tbižūz\tčaẓbeṭ\n",
      "zal-lun\tUNK\tḥim-l\thanna\tpsōn-a\txann\tamel-lun\thanna\tmō\tčūl=e\ttwō\tbess\tn-ōz\tUNK\twaṣəf-ṯ-a\tUNK\tč-aẓbeṭ\n",
      "STEM.PST-3mp\tUNK\tSTEM.PST-DOM\tthis.M\tSTEM-FREE\tlike.so\tSTEM.PST-3mp.IO\tthis.M\twhat\tSTEM=3ms\tSTEM\tSTEM\t1-STEM.PRS\tUNK\tSTEM-F-FREE\tUNK\t2-STEM.SBJV\n",
      "\n",
      "hōḏ\thī\tḳeṣṣṯa\tnčahyaṯ\n",
      "hōḏ\thī\tḳeṣṣ-ṯ-a\tUNK\n",
      "this.F\tshe\tSTEM-F-FREE\tUNK\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(gloss_text(\"text_to_gold.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation tool for glossing tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation tool is also a function. It takes two `.txt` files, the first one is glossed text which need to be checked and the second one is glossed text which represents a gold standard.  \n",
    "If you want to display which words were analysed incorrectly, you can specify the parameter `view_incorrect=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_glossed_text(check_text, gold_text, view_incorrect=False):\n",
    "    i=0\n",
    "    glossed=[]\n",
    "    glosses=[]\n",
    "    gold_glossed=[]\n",
    "    gold_glosses=[]\n",
    "\n",
    "    f=open(check_text, \"r\", encoding=\"utf-8\")\n",
    "    g=open(gold_text, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    for line in f:\n",
    "        if i % 3 == 0:\n",
    "            glossed.append(line[:-1].split(\"\\t\"))\n",
    "        if i % 3 == 1:\n",
    "            glosses.append(line[:-1].split(\"\\t\"))\n",
    "        i+=1\n",
    "    \n",
    "    i=0\n",
    "    for line in g:\n",
    "        if i % 3 == 0:\n",
    "            gold_glossed.append(line[:-1].split(\"\\t\"))\n",
    "        if i % 3 == 1:\n",
    "            gold_glosses.append(line[:-1].split(\"\\t\"))\n",
    "        i+=1\n",
    "\n",
    "    f.close()\n",
    "    g.close()\n",
    "\n",
    "\n",
    "    n=len(glossed)\n",
    "    divisions1=0\n",
    "    divisions2=0\n",
    "    divisions3=0\n",
    "    divisions4=0\n",
    "    gloss_choice1=0\n",
    "    gloss_choice2=0\n",
    "    gloss_choice3=0\n",
    "    gloss_choice4=0\n",
    "    total_words1=0\n",
    "    total_words2=0\n",
    "    total_words3=0\n",
    "    total_words4=0\n",
    "    all_words=[]\n",
    "\n",
    "\n",
    "    for q in range(n):\n",
    "        for p in range(len(glossed[q])):\n",
    "            if \"UNK\" not in glossed[q][p]:\n",
    "                if gold_glossed[q][p] not in all_words:\n",
    "                    all_words.append(gold_glossed[q][p])\n",
    "                    total_words1+=1\n",
    "                    total_words2+=1\n",
    "                    total_words3+=1\n",
    "                    total_words4+=1\n",
    "                    if glossed[q][p]==gold_glossed[q][p]:\n",
    "                        divisions1+=1\n",
    "                        divisions2+=1\n",
    "                        divisions3+=1\n",
    "                        divisions4+=1\n",
    "                    else:\n",
    "                        if view_incorrect==True:\n",
    "                            print(glossed[q][p], \"\\t\", gold_glossed[q][p])\n",
    "                    if glosses[q][p]==gold_glosses[q][p]:\n",
    "                        gloss_choice1+=1\n",
    "                        gloss_choice2+=1\n",
    "                        gloss_choice3+=1\n",
    "                        gloss_choice4+=1\n",
    "                    else:\n",
    "                        if view_incorrect==True:\n",
    "                            print(glosses[q][p], \"\\t\", gold_glosses[q][p])\n",
    "                else:\n",
    "                    total_words2+=1\n",
    "                    if glossed[q][p]==gold_glossed[q][p]:\n",
    "                        divisions2+=1\n",
    "                    else:\n",
    "                        if view_incorrect==True:\n",
    "                            print(glossed[q][p], \"\\t\", gold_glossed[q][p])\n",
    "                    if glosses[q][p]==gold_glosses[q][p]:\n",
    "                        gloss_choice2+=1\n",
    "                    else:\n",
    "                        if view_incorrect==True:\n",
    "                            print(glosses[q][p], \"\\t\", gold_glosses[q][p])\n",
    "            \n",
    "            else:\n",
    "                if gold_glossed[q][p] not in all_words:\n",
    "                    all_words.append(glossed[q][p])\n",
    "                    total_words3+=1\n",
    "                    total_words4+=1\n",
    "                    if glossed[q][p]==gold_glossed[q][p]:\n",
    "                        divisions3+=1\n",
    "                        divisions4+=1\n",
    "                    else:\n",
    "                        if view_incorrect==True:\n",
    "                            print(glossed[q][p], \"\\t\", gold_glossed[q][p])\n",
    "                    if glosses[q][p]==gold_glosses[q][p]:\n",
    "                        gloss_choice3+=1\n",
    "                        gloss_choice4+=1\n",
    "                    else:\n",
    "                        if view_incorrect==True:\n",
    "                            print(glosses[q][p], \"\\t\", gold_glosses[q][p])\n",
    "                else:\n",
    "                    total_words3+=1\n",
    "                    if glossed[q][p]==gold_glossed[q][p]:\n",
    "                        divisions3+=1\n",
    "                    else:\n",
    "                        if view_incorrect==True:\n",
    "                            print(glossed[q][p], \"\\t\", gold_glossed[q][p])\n",
    "                    if glosses[q][p]==gold_glosses[q][p]:\n",
    "                        gloss_choice3+=1\n",
    "                    else:\n",
    "                        if view_incorrect==True:\n",
    "                            print(glosses[q][p], \"\\t\", gold_glosses[q][p])\n",
    "\n",
    "    \n",
    "    result1=\"\\nAnalysed wordforms\\nCorrect divisions: \" + str(round(divisions1*100/total_words1, 3)) + \"%\\n\"+\"Correct glosses: \" + str(round(gloss_choice1*100/total_words1, 3)) + \"%\"\n",
    "    result2=\"\\nAnalysed unique wordforms\\nCorrect divisions: \" + str(round(divisions2*100/total_words2, 3)) + \"%\\n\"+\"Correct glosses: \" + str(round(gloss_choice2*100/total_words2, 3)) + \"%\"\n",
    "    result3=\"\\nAll wordforms\\nCorrect divisions: \" + str(round(divisions3*100/total_words3, 3)) + \"%\\n\"+\"Correct glosses: \" + str(round(gloss_choice3*100/total_words3, 3)) + \"%\"\n",
    "    result4=\"\\nAll unique wordforms\\nCorrect divisions: \" + str(round(divisions4*100/total_words4, 3)) + \"%\\n\"+\"Correct glosses: \" + str(round(gloss_choice4*100/total_words4, 3)) + \"%\"\n",
    " \n",
    "    return result1+\"\\n\"+result2+\"\\n\"+result3+\"\\n\"+result4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysed wordforms\n",
      "Correct divisions: 89.963%\n",
      "Correct glosses: 76.208%\n",
      "\n",
      "Analysed unique wordforms\n",
      "Correct divisions: 91.169%\n",
      "Correct glosses: 78.043%\n",
      "\n",
      "All wordforms\n",
      "Correct divisions: 73.556%\n",
      "Correct glosses: 62.31%\n",
      "\n",
      "All unique wordforms\n",
      "Correct divisions: 74.006%\n",
      "Correct glosses: 62.691%\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_glossed_text(\"glosses_output.txt\", \"glosses_gold.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
